# InferenceEngine

A lightweight inference engine that integrates with [llama.cpp](https://github.com/ggml-org/llama.cpp) for efficient model execution.

## üöÄ Setup

To ensure compatibility, this project uses **llama.cpp version b4514**``.

### 1Ô∏è‚É£ Clone the Repository

```sh
cd /path/to/github/directory
```

### 2Ô∏è‚É£ Setup `llama.cpp`

```sh
cd external
git clone https://github.com/ggml-org/llama.cpp.git
cd llama.cpp
git checkout b4514
git switch -c b4514
git branch  # Ensure the branch is `b4514` or lower
git submodule update --init --recursive
git lfs install
git lfs pull
```

## üõ†Ô∏è Build Instructions

### 1Ô∏è‚É£ Create Build Directory

```sh
cd ../..
mkdir build && cd build
```

### 2Ô∏è‚É£ Compile the Project

```sh
cmake ..
make
```

## ‚úÖ Running Tests

After a successful build, run the test binary:

```sh
./bin/test
```

